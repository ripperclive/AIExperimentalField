import chat from "./components/chat.js";
import chatPrompt from "./components/template.js";
import { PromptTemplate } from "langchain/prompts";

// ç¼“å­˜
import { BufferMemory } from "langchain/memory";
import { LLMChain } from "langchain/chains";
import { ConversationChain } from "langchain/chains";
import { BufferWindowMemory } from "langchain/memory";
import { ConversationSummaryMemory } from "langchain/memory";
// openAIå®˜æ–¹æä¾›çš„æ–‡æœ¬åˆ†æå™¨ï¼Œå¯ä»¥å°†æ–‡æœ¬ç”¨å‘é‡æ¥è¡¨ç¤º,ä¹Ÿå¯ä»¥ç”¨æ¥ç”Ÿæˆæ‘˜è¦ã€‚ éœ€è¦KEY
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { VectorStoreRetrieverMemory } from "langchain/memory";


// newä¸€ä¸ª
const memory = new BufferMemory({ returnMessages: true, memoryKey: "history" })

// å› ä¸ºä½¿ç”¨çš„æ˜¯ConversationChainï¼Œå¯ä»¥é¡ºè·¯æŠŠæ¨¡æ¿ä¹Ÿå¡è¿›å»
const memoryChain = new ConversationChain({
    memory: memory,
    prompt: chatPrompt,
    llm: chat,
})
//------------------------------------------------------------------------------------------------------------

// å¦‚æœé¢‘ç¹èŠå¤©å ç”¨å†…å­˜è¿‡å¤§ï¼Œä¹Ÿå¯ä»¥é™åˆ¶çª—å£å¤§å°
// k:1 è¡¨ç¤ºé™åˆ¶çª—å£å¤§å°ä¸ºä¸€æ¬¡ä¼šè¯
const restrictMemory = new BufferWindowMemory({ k: 1, returnMessages: true, memoryKey: "history" })
const restrictMemoryChain = new ConversationChain({
    memory: restrictMemory,
    prompt: chatPrompt,
    llm: chat,
})
//------------------------------------------------------------------------------------------------------------
// å¦‚æœè¾“å…¥ä¸€äº›å¤æ‚çš„ç»„åˆä¿¡æ¯ï¼Œå¯ä»¥é€‰æ‹©è®©AIå»ç¼“å­˜å†å²ä¿¡æ¯çš„æ‘˜è¦ï¼Œè€Œä¸æ˜¯å»æ­»è®°ç¡¬èƒŒèŠå¤©è®°å½•
// éœ€è¦é…åˆæ¨¡æ¿ä¸€èµ·ä½¿ç”¨
const intelligenceMemory = new ConversationSummaryMemory({
    memoryKey: "chat_history",
    llm: chat
});

const prompt =
    PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
  Current conversation:
  {chat_history}
  Human: {input}
  AI:`);
const intelligenceMemoryChain = new LLMChain({ llm: chat, prompt, memory: intelligenceMemory });
/**
 * æ—¢ç„¶AIä¼šæ•´åˆèŠå¤©è®°å½•ï¼Œå¹¶ä¸”ç»„ç»‡æˆä¸€ä¸ªä¸Šä¸‹æ–‡ï¼Œå¦‚æœåœ¨ç”¨æˆ·å’ŒAIé€šè¯å‰ä½¿ç”¨SystemChatMessageæå‰å’ŒAIæ‰“å¥½æ‹›å‘¼é“ºå«ä¸€ä¸‹ä¸Šä¸‹æ–‡ï¼Ÿ
 *     ï¼ˆğŸ˜…é‚£ä¸ºå•¥ä¸ç›´æ¥æ‹¼æ¨¡æ¿å•Šï¼Ÿï¼‰
 *     const response = await chat.call([
 *     new SystemChatMessage(
 *         'æˆ‘å«éœ²ä¸ï¼Œæˆ‘çš„ç”Ÿæ—¥æ˜¯4æœˆ30æ—¥'
 *     ),
 *     new HumanChatMessage(
 *         { input: inputValue }
 *     )
 * ])
 * ç„¶è€Œå¹¶ä¸è¡Œï¼Œç›¸å…³åŠŸèƒ½è‚¯å®šæ˜¯æœ‰ï¼Œä½†æ˜¯ä¸èƒ½è¿™ä¹ˆå†™
*/


// ç¼“å­˜çš„å†…å®¹æ˜¯å­˜å‚¨åœ¨å†…å­˜ä¸­çš„ï¼Œå¦‚æœéœ€è¦æŒä¹…åŒ–ä¿å­˜ï¼Œå®˜ç½‘ç»™çš„ä¾‹å­ä½¿ç”¨Motorhead
// æˆ‘æ²¡å†™è¿‡åç«¯ï¼Œä¸çŸ¥é“ç”¨æˆ·çš„èŠå¤©è®°å½•æˆ–è€…èŠå¤©æ‘˜è¦å…¨éƒ¨å­˜åœ¨ç¼“å­˜é‡Œä¼šä¸ä¼šç»™æœåŠ¡å™¨å¹²ğŸ’¥ï¼Œé‰´äºå­¦ä¹ æˆæœ¬è¿™å—å…ˆç•¥è¿‡
//------------------------------------------------------------------------------------------------------------


// ä¹Ÿå¯ä»¥å°†å¯¹è¯æ•°æ®å­˜å‚¨åœ¨VectorDBä¸­ï¼ŒVectorDBæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å‘é‡æ£€ç´¢åº“ï¼ŒåŸºäºç±»ä¼¼HNSWçš„ç®—æ³•æ¥è¿›è¡Œå¿«é€Ÿçš„ç›¸ä¼¼åº¦æœç´¢
// HNSWç›¸å…³çš„åº“åé¢ä¹Ÿä¼šç”¨åˆ°ï¼Œæ¯”å¦‚çŸ¥è¯†åº“
const vectorStore = new MemoryVectorStore(new OpenAIEmbeddings({ openAIApiKey: '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' }))
// 
const vectorStoreMemory = new VectorStoreRetrieverMemory({
    // 1æ˜¯è¦å›æº¯çš„æ–‡æœ¬/å¯¹è¯æ•°é‡
    vectorStoreRetriever: vectorStore.asRetriever(1),
    memoryKey: "history",
});



// åœ¨å¼€å§‹ä¹‹å‰å¯ä»¥å…ˆå°†ä¸€äº›ä¿¡æ¯ä¿å­˜åœ¨ç¼“å­˜ä¸­
await vectorStoreMemory.saveContext(
    { input: "æˆ‘çš„æ€§åˆ«æ˜¯æ­¦è£…ç›´å‡æœº" },
    { output: "å¤©å“ªï¼Œä½ ä¸€å®šæ˜¯ä¸ªç¾å›½äºº" }
)
await vectorStoreMemory.saveContext(
    { input: "ä½ å¥½,æˆ‘çš„åå­—å«jim" },
    { output: "æ˜¯çš„,ä½ å«jim" }
)
await vectorStoreMemory.saveContext(
    { input: "æˆ‘æœ€å–œæ¬¢çš„åŠ¨ç‰©æ˜¯äºº" },
    { output: "é‚£å¤ªå¥½äº†" }
)
await vectorStoreMemory.saveContext(
    { input: "æˆ‘æœ€å–œæ¬¢çš„ä»£æ­¥å·¥å…·æ˜¯AC-130" },
    { output: "é‚£ä½ ä¸€å®šå¾ˆæœ‰é’±" }
)
// è¾“å‡ºä¸€ä¸‹çœ‹çœ‹æ•ˆæœ
// console.log(
//     await vectorStoreMemory.loadMemoryVariables({ prompt: "æˆ‘çš„æ€§åˆ«æ˜¯ä»€ä¹ˆ" })
// )
// { history: 'input: æˆ‘çš„æ€§åˆ«æ˜¯æ­¦è£…ç›´å‡æœº\noutput: â€¦â€¦' }

const vectorPrompt =
    PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Relevant pieces of previous conversation:
{history}

(You do not need to use these pieces of information if not relevant)

Current conversation:
Human: {input}
AI:`);

const vectorChain = new LLMChain({ llm: chat, prompt:vectorPrompt, memory:vectorStoreMemory });


/**
 * memoryChain: æ™®é€šç¼“å­˜
 * restrictMemoryChain: é™åˆ¶èŠå¤©çª—å£
 * intelligenceMemoryChain: æ™ºèƒ½ç¼“å­˜ï¼Œæ ¹æ®èŠå¤©å†…å®¹ç”Ÿæˆæ‘˜è¦
 * vectorChain: ä½¿ç”¨å‘é‡åº“
 */
async function memoryChat(inputValue) {

    const response = await memoryChain.call({ input: inputValue })
    console.log(response)
    // console.log({ 'AIçš„å†…å¿ƒOS': await intelligenceMemory.loadMemoryVariables({}) })

}



export default memoryChat